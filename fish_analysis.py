# -*- coding: utf-8 -*-
"""Fish Analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DHzGQhWBjJYFf7mrd6vuFEkZcGVLqDd5
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing

print(tf.__version__)

url ='http://drnam.org/pythonfiles/Fish.csv'
column_names =['Species','Weight','Length1','Length2','Length3','Height','Width']

raw_dataset = pd.read_csv(url)
dataset = raw_dataset.copy()
dataset.tail()

dataset.isna() .sum()

dataset = dataset.dropna()

dataset['Species'] = dataset['Species'].map({'Bream': 1, 'Parkki': 2, 'Perch': 3, 'Pike': 4, 'Roach': 5, 'Smelt': 6, 'Whitefish': 7})
dataset.tail()

train_dataset = dataset.sample(frac=0.8, random_state=0)
test_dataset = dataset.drop(train_dataset.index)
train_dataset

sns.pairplot(train_dataset[['Species', 'Weight', 'Length1', 'Length2', 'Length3', 'Height', 'Width']], diag_kind='kde')

train_dataset.describe().transpose()

train_features = train_dataset.copy()
test_features = test_dataset.copy()

train_labels = train_features.pop('Weight')
test_labels = test_features.pop('Weight')
train_features.tail()

normalizer = preprocessing.Normalization()
normalizer.adapt(np.array(train_features))
print(normalizer.mean.numpy())

first = np.array(train_features[:1])

with np.printoptions(precision=2, suppress=True):
  print('First example:', first)
  print()
  print('Normalized:', normalizer(first).numpy())

length1 = np.array(train_features['Length1'])
length1_normalizer = preprocessing.Normalization(input_shape=[1,])
length1_normalizer.adapt(length1)

length1_model = tf.keras.Sequential([
    length1_normalizer,
    layers.Dense(units=1)
])

length1_model.summary()

length1_model.predict(length1[:10])

length1_model.compile(
    optimizer=tf.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = length1_model.fit(
#     train_features['Length1'], train_labels,
#     epochs=100,
#     # suppress logging
#     verbose=0,
#     # Calculate validation results on 20% of the training data
#     validation_split = 0.2)

hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()

def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
 # plt.ylim([0, 10])
  plt.xlabel('Epoch')
  plt.ylabel('Error [Weight]')
  plt.legend()
  plt.grid(True)

plot_loss(history)

test_results = {}

test_results['length1_model'] = length1_model.evaluate(
    test_features['Length1'],
    test_labels, verbose=0)
print(test_results)

x = tf.linspace(0.0, 250, 251)
y = length1_model.predict(x)

def plot_length1(x, y):
  plt.scatter(train_features['Length1'], train_labels, label='Data')
  plt.plot(x, y, color='k', label='Predictions')
  plt.xlabel('Length1')
  plt.ylabel('Weight')
  plt.legend()

plot_length1(x,y)

linear_model = tf.keras.Sequential([
    normalizer,
    layers.Dense(units=1)
])

linear_model.predict(train_features[:10])

linear_model.layers[1].kernel

linear_model.compile(
    optimizer=tf.optimizers.Adam(learning_rate=0.1),
    loss='mean_absolute_error')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = linear_model.fit(
#     train_features, train_labels, 
#     epochs=100,
#     # suppress logging
#     verbose=0,
#     # Calculate validation results on 20% of the training data
#     validation_split = 0.2)

plot_loss(history)

test_results['linear_model'] = linear_model.evaluate(
    test_features, test_labels, verbose=0)
print(test_results)

def build_and_compile_model(norm):
  model = keras.Sequential([
      norm,
      layers.Dense(64, activation='relu'),
      layers.Dense(64, activation='relu'),
      layers.Dense(1)
  ])

  model.compile(loss='mean_absolute_error',
                optimizer=tf.keras.optimizers.Adam(0.001))
  return model

dnn_length1_model = build_and_compile_model(length1_normalizer)

dnn_length1_model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = dnn_length1_model.fit(
#     train_features['Length1'], train_labels,
#     validation_split=0.2,
#     verbose=0, epochs=100)

plot_loss(history)

x = tf.linspace(0.0, 80, 81)
y = dnn_length1_model.predict(x)

plot_length1(x, y)

test_results['dnn_length1_model'] = dnn_length1_model.evaluate(
    test_features['Length1'], test_labels,
    verbose=0)

dnn_model = build_and_compile_model(normalizer)
dnn_model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = dnn_model.fit(
#     train_features, train_labels,
#     validation_split=0.2,
#     verbose=0, epochs=100)

plot_loss(history)

test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)

pd.DataFrame(test_results, index=['Mean absolute error [Weight]']).T

test_predictions = dnn_model.predict(test_features).flatten()

a = plt.axes(aspect='equal')
plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values [Weight]')
plt.ylabel('Predictions [Weight]')
xlims = [0, 1000]
ylims = [0, 1000]
plt.xlim(xlims)
plt.ylim(ylims)
_ = plt.plot(xlims, ylims)

error = test_predictions - test_labels
plt.hist(error, bins=25)
plt.xlabel('Prediction Error [Weight]')
_ = plt.ylabel('Count')

dnn_model.save('dnn_model')